# ğŸ“ University FAQ Assistant

A production-ready RAG (Retrieval-Augmented Generation) chatbot that answers student questions using official university documents like handbooks, course catalogs, and academic policies.

![Status](https://img.shields.io/badge/status-production--ready-brightgreen)
![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)

## ğŸ“‹ Table of Contents
- [Features](#-features)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Setup Instructions](#-setup-instructions)
- [How to Use](#-how-to-use)
- [How RAG Works](#-how-rag-works)
- [Document Compression](#-document-compression)
- [Configuration](#-configuration)
- [Troubleshooting](#-troubleshooting)
- [Evaluation Checklist](#-evaluation-checklist)

## âœ¨ Features

- **RAG-based Question Answering**: Retrieves relevant information from university documents before generating answers
- **Document Compression**: Implements text summarization before embedding (ScaleDown technique) for efficient storage and faster retrieval
- **Multi-document Support**: Processes multiple PDF documents (handbooks, catalogs, policies, calendars)
- **Source Attribution**: Shows source documents and page numbers for transparency
- **Web Interface**: Clean, user-friendly Streamlit interface
- **Configurable LLM**: Supports both OpenAI (paid) and HuggingFace (free) models
- **Local Deployment**: Runs completely on your machine
- **Production Ready**: Includes error handling, logging, and validation

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â”‚  Question   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAG Pipeline                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. Question Embedding          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 2. FAISS Similarity Search     â”‚ â”‚
â”‚  â”‚    (Retrieve Top-K Chunks)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 3. Context Preparation         â”‚ â”‚
â”‚  â”‚    (Use Original Content)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 4. LLM Answer Generation       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Answer      â”‚
â”‚   + Sources     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data Flow (Ingestion):
PDF â†’ Extract Text â†’ Chunk â†’ Summarize/Compress â†’ Embed â†’ FAISS
```

## ğŸ“ Project Structure

```
university_faq_assistant/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ config.py              # Configuration and settings
â”‚   â”œâ”€â”€ utils.py               # Helper functions
â”‚   â”œâ”€â”€ ingest.py              # Document ingestion pipeline
â”‚   â””â”€â”€ rag_pipeline.py        # RAG implementation
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                 # Streamlit web interface
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/                  # Place your PDF documents here
â”‚   â””â”€â”€ vector_db/             # FAISS index (auto-generated)
â”‚
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # This file
â””â”€â”€ .env.example              # Environment variables template
```

## ğŸš€ Setup Instructions

### Prerequisites

- Python 3.8 or higher
- pip (Python package manager)
- 4GB+ RAM recommended
- OpenAI API key (optional, for best results) OR use free HuggingFace models

### Step 1: Clone/Download the Project

```bash
cd "d:\Intell Unnati\university_faq_assistant"
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Windows
python -m venv venv
.\venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Note**: Installation may take 5-10 minutes due to ML libraries.

### Step 4: Configure Environment

```bash
# Copy the example environment file
copy .env.example .env

# Edit .env file with your configuration
notepad .env
```

**Configuration Options:**

**Option A: Using OpenAI (Best Quality)**
```env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-actual-api-key-here
```

**Option B: Using HuggingFace (Free)**
```env
LLM_PROVIDER=huggingface
HUGGINGFACE_API_KEY=        # Optional, leave empty for local models
```

### Step 5: Add University Documents

Place your PDF documents in `data/pdfs/` directory:

```bash
university_faq_assistant/
â””â”€â”€ data/
    â””â”€â”€ pdfs/
        â”œâ”€â”€ student_handbook.pdf
        â”œâ”€â”€ course_catalog.pdf
        â”œâ”€â”€ academic_policies.pdf
        â””â”€â”€ academic_calendar.pdf
```

**Sample Documents** (if you don't have real ones):
- You can use any PDF documents for testing
- Create sample PDFs with university-related content
- Or download sample university handbooks from public sources

### Step 6: Run Document Ingestion

This processes PDFs, compresses content, and creates the vector database:

```bash
python backend/ingest.py
```

**Expected Output:**
```
============================================================
ğŸš€ STARTING DOCUMENT INGESTION PIPELINE
============================================================

ğŸ“ Found 4 PDF file(s) to process

ğŸ“„ Extracting text from PDFs...
   Processing: student_handbook.pdf
   Processing: course_catalog.pdf
   ...

âœ… Extracted text from 156 pages
âœ‚ï¸ Chunking documents...
   Created 312 chunks from 156 pages

ğŸ—œï¸ Compressing chunks (this may take a while)...
Compressing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 312/312 [02:15<00:00,  2.31it/s]
   Average compression ratio: 0.53

ğŸ”¢ Creating embeddings and building FAISS index...
âœ… Successfully created vector store with 312 chunks
ğŸ’¾ Vector store saved to: data/vector_db/faiss_index

============================================================
âœ… INGESTION COMPLETE!
============================================================
ğŸ“Š Summary:
   - PDF files processed: 4
   - Total pages: 156
   - Chunks created: 312
   - Compression enabled: True
   - Vector store location: data/vector_db/faiss_index

âœ¨ You can now run the chatbot application!
```

**Note**: First-time ingestion may take 5-15 minutes depending on document size and whether you're using compression.

### Step 7: Launch the Application

```bash
streamlit run frontend/app.py
```

The application will open in your browser at `http://localhost:8501`

## ğŸ“– How to Use

### Web Interface

1. **Start the app**: `streamlit run frontend/app.py`
2. **Ask questions**: Type your question in the input box
3. **View answers**: AI generates answers based on your documents
4. **Check sources**: Click "View Sources" to see relevant document excerpts
5. **Use examples**: Click sidebar example questions for quick testing

### Example Questions

- "What are the admission requirements?"
- "When does the fall semester start?"
- "What is the refund policy?"
- "How do I register for courses?"
- "What are the graduation requirements?"
- "Tell me about academic probation"

### Command-Line Testing

You can also test the RAG pipeline directly:

```bash
python backend/rag_pipeline.py
```

## ğŸ” How RAG Works

**RAG (Retrieval-Augmented Generation)** improves AI responses by grounding them in your actual documents:

### Traditional Approach (No RAG)
```
User Question â†’ LLM â†’ Answer (may hallucinate or be outdated)
```

### RAG Approach (This Project)
```
User Question â†’ Vector Search â†’ Retrieve Relevant Docs â†’ LLM + Context â†’ Accurate Answer
```

### Our Implementation

1. **Ingestion Phase** (One-time setup):
   - Load PDF documents
   - Extract and clean text
   - Split into manageable chunks (1000 characters)
   - **Compress chunks** via summarization (ScaleDown)
   - Create embeddings (vector representations)
   - Store in FAISS vector database

2. **Query Phase** (Each question):
   - Convert question to embedding
   - Find Top-K most similar chunks in FAISS
   - Retrieve original (uncompressed) content
   - Provide context to LLM
   - Generate answer based ONLY on retrieved context

### Benefits

- âœ… **Accurate**: Answers from your documents, not general knowledge
- âœ… **Traceable**: Shows source documents for verification
- âœ… **Up-to-date**: Uses your latest documents
- âœ… **No hallucination**: Won't make up information

## ğŸ—œï¸ Document Compression

### What is Document Compression?

Document compression (also called "ScaleDown" or chunk summarization) reduces storage requirements and improves retrieval speed by summarizing document chunks before creating embeddings.

### How It Works in This Project

1. **Original Chunk** (1000 characters):
   ```
   "The university requires all incoming freshmen to submit the following 
   documents: high school transcript, SAT/ACT scores, two letters of 
   recommendation, a personal statement (500-750 words), and proof of 
   English proficiency for international students. The application 
   deadline for fall admission is January 15th. Early decision 
   applications are due November 1st..."
   ```

2. **Compressed Summary** (~500 characters):
   ```
   "Freshmen admission requirements: high school transcript, SAT/ACT 
   scores, two recommendation letters, personal statement (500-750 words), 
   English proficiency proof for international students. Fall deadline: 
   January 15th. Early decision: November 1st."
   ```

3. **Storage**:
   - Compressed version â†’ Used for embedding and search
   - Original version â†’ Stored in metadata, used for answer generation

### Benefits

- **Faster Retrieval**: Smaller embeddings = faster similarity search
- **Better Matching**: Summaries capture key concepts more clearly
- **Reduced Storage**: ~50% reduction in vector database size
- **Maintained Quality**: Original content used for final answers

### Configuration

Enable/disable compression in `.env` or `config.py`:

```python
ENABLE_COMPRESSION = True  # Set to False to disable
COMPRESSION_RATIO = 0.5    # Target ratio (50% reduction)
```

## âš™ï¸ Configuration

### Key Settings in `backend/config.py`

```python
# Text Chunking
CHUNK_SIZE = 1000           # Characters per chunk
CHUNK_OVERLAP = 200         # Overlap between chunks

# Compression
ENABLE_COMPRESSION = True   # Enable/disable summarization
COMPRESSION_RATIO = 0.5     # Target compression ratio

# RAG Settings
TOP_K_RETRIEVAL = 4         # Number of chunks to retrieve
GENERATION_TEMPERATURE = 0.3 # LLM creativity (0=deterministic, 1=creative)
MAX_OUTPUT_TOKENS = 500     # Maximum answer length
```

### Model Selection

**OpenAI Models** (Paid, High Quality):
- `gpt-3.5-turbo` - Fast, cost-effective
- `gpt-4` - Highest quality (slower, more expensive)

**HuggingFace Models** (Free):
- `google/flan-t5-base` - Good balance
- `google/flan-t5-large` - Better quality (requires more RAM)
- `tiiuae/falcon-7b-instruct` - Advanced (requires GPU)

## ğŸ”§ Troubleshooting

### Issue: "Vector store not found"

**Solution**: Run document ingestion first:
```bash
python backend/ingest.py
```

### Issue: "No PDF files found"

**Solution**: Add PDF files to `data/pdfs/` directory.

### Issue: "OpenAI API error"

**Solutions**:
1. Check API key in `.env` file
2. Verify API key is active at https://platform.openai.com/api-keys
3. Check you have credits in your account
4. OR switch to HuggingFace: `LLM_PROVIDER=huggingface`

### Issue: "Out of memory"

**Solutions**:
1. Reduce `CHUNK_SIZE` in config.py
2. Enable compression: `ENABLE_COMPRESSION=True`
3. Use smaller model: `google/flan-t5-base`
4. Process fewer PDFs at once

### Issue: "Slow response time"

**Solutions**:
1. Use OpenAI instead of local HuggingFace models
2. Reduce `TOP_K_RETRIEVAL` (e.g., from 4 to 2)
3. Enable compression if not already on
4. Use GPU if available (install `faiss-gpu` instead of `faiss-cpu`)

### Issue: "Poor answer quality"

**Solutions**:
1. Use OpenAI models (better than free alternatives)
2. Increase `TOP_K_RETRIEVAL` to provide more context
3. Ensure PDFs have clear, well-formatted text
4. Adjust `CHUNK_SIZE` for better granularity
5. Check that questions match document content

## âœ… Evaluation Checklist

Use this checklist to verify the project works correctly:

### Setup & Installation
- [ ] Python 3.8+ installed
- [ ] Virtual environment created and activated
- [ ] All dependencies installed (`pip install -r requirements.txt`)
- [ ] `.env` file configured with API keys
- [ ] PDF documents placed in `data/pdfs/` directory

### Document Ingestion
- [ ] `python backend/ingest.py` runs without errors
- [ ] Console shows PDF processing progress
- [ ] Compression statistics displayed
- [ ] FAISS index created in `data/vector_db/`
- [ ] Summary shows correct document count

### RAG Pipeline Test
- [ ] `python backend/rag_pipeline.py` runs successfully
- [ ] Sample questions get answered
- [ ] Answers are relevant to documents

### Web Interface
- [ ] `streamlit run frontend/app.py` launches successfully
- [ ] Browser opens to application
- [ ] Sidebar shows "Vector store loaded" status
- [ ] Correct document count displayed
- [ ] Can input questions
- [ ] Receives relevant answers
- [ ] "View Sources" shows source documents
- [ ] Example questions work
- [ ] Clear history button works

### RAG Functionality
- [ ] Answers are based on provided documents
- [ ] Says "not available" when info isn't in docs
- [ ] Source attribution works correctly
- [ ] Multiple document sources handled properly

### Compression Verification
- [ ] Compression enabled in config
- [ ] Ingestion shows compression statistics
- [ ] Compression ratio approximately 0.5
- [ ] Answer quality maintained

### Additional Checks
- [ ] Academic calendar questions work
- [ ] No error messages in console
- [ ] Response time is acceptable (<10 seconds)
- [ ] Code includes inline comments
- [ ] README instructions are clear

## ğŸ“š Technologies Used

- **LangChain**: RAG framework and orchestration
- **FAISS**: Vector similarity search
- **OpenAI/HuggingFace**: Language models and embeddings
- **Streamlit**: Web interface
- **PyPDF**: PDF text extraction
- **Transformers**: NLP models

## ğŸ¯ Project Highlights for Assessment

This project demonstrates:

1. **RAG Implementation**: Complete retrieval-augmented generation pipeline
2. **Document Compression**: ScaleDown technique via summarization
3. **Production Quality**: Error handling, logging, configuration management
4. **Scalability**: Processes multiple documents efficiently
5. **User Experience**: Clean, intuitive web interface
6. **Flexibility**: Configurable for different LLM providers
7. **Documentation**: Comprehensive README with clear instructions

## ğŸ“ License

MIT License - Feel free to use for educational purposes.

## ğŸ‘¨â€ğŸ’» Author

Created for Intel Unnati Assessment - Intermediate Level (1-week project)

---

**Need Help?** Check the [Troubleshooting](#-troubleshooting) section or review error messages in the console for detailed debugging information.
